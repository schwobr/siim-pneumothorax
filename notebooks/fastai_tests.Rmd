---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.3
  kernelspec:
    display_name: Python [conda env:pytorch] *
    language: python
    name: conda-env-pytorch-py
---

```{python}
import cv2
import PIL
import random
import numpy as np
import os
import pydicom
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm_notebook as tqdm
import pdb

from skimage.morphology import label

from torchvision import transforms
import torchvision.transforms.functional as TF
from torch.nn.functional import binary_cross_entropy_with_logits
import torch
import torch.nn as nn
from torch.utils.data import WeightedRandomSampler, Sampler

from fastai.vision.data import SegmentationItemList, SegmentationLabelList, ImageList
from fastai.data_block import FloatList, FloatItem
from fastai.basic_data import DatasetType
from fastai.vision.image import Image, ImageSegment, image2np, pil2tensor
from fastai.vision.transform import get_transforms
from fastai.vision.learner import unet_learner, cnn_learner
import fastai.vision.models as mod
from fastai.callbacks import SaveModelCallback
from fastai.metrics import accuracy

from pathlib import Path

# IMAGE SIZES
TRAIN_SIZE = 256
MAX_SIZE = 1388
TEST_SIZE = 224
TEST_OVERLAP = 64
IMG_CHANNELS = 3

# PATHS
PROJECT_PATH = Path(
    '/work/stages/schwob/siim-pneumothorax')
FULL_TRAIN_PATH = PROJECT_PATH/'data/dicom-images-train'
FULL_TEST_PATH = PROJECT_PATH/'data/dicom-images-test'
DATA = PROJECT_PATH/'data'
TRAIN_PATH = PROJECT_PATH/'data/train'
TEST_PATH = PROJECT_PATH/'data/test'
MODELS_PATH = PROJECT_PATH/'models/'
STATE_DICT_PATH = MODELS_PATH/'resnet152_backbone_pretrained.pth'
SUB_PATH = PROJECT_PATH/'submissions/'
LABELS_OLD = PROJECT_PATH/'data/train-rle.csv'
LABELS = PROJECT_PATH/'data/train-rle-fastai2.csv'
LABELS_POS = PROJECT_PATH/'data/train-rle-fastai_pos.csv'
LABELS_CLASSIF = PROJECT_PATH/'data/train-rle-fastai-classif.csv'
LOG = Path('/work/stages/schwob/runs')

# LEARNER CONFIG
BATCH_SIZE = 4
WD = 0.1
LR = 2e-4
GROUP_LIMITS = None
FREEZE_UNTIL = None
EPOCHS = 10
UNFROZE_EPOCHS = 10
PRETRAINED = True
MODEL = 'resnet152'
CLASSES = ['pneum']
ACT = 'sigmoid'
```

```{python}
models = {
    'resnet34': mod.resnet34, 'resnet50': mod.resnet50,
    'resnet101': mod.resnet101, 'resnet152': mod.resnet152}
```

```{python}
def restruct(src, dest):
    for fn in src.glob('**/*dcm'):
        ds = pydicom.dcmread(str(fn))
        pydicom.dcmwrite(str(dest/fn.name), ds)
```

```{python}
#restruct(FULL_TRAIN_PATH, TRAIN_PATH)
```

```{python}
#restruct(FULL_TEST_PATH, TEST_PATH)
```

```{python}
def change_csv(old, new, path):
    df = pd.read_csv(old, sep=', ')
    new_df = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])
    for row in df.itertuples():
        image_id = row.ImageId
        label = row.EncodedPixels
        image_id = Path(path.name)/(image_id+'.dcm')
        new_df.loc[row.Index] = [image_id, label]
    new_df.to_csv(new, index=False)
```

```{python}
#change_csv(LABELS_OLD, LABELS, TRAIN_PATH)
```

```{python}
df = pd.read_csv(LABELS, header='infer')
df.head()
```

```{python}
df.shape
```

```{python}
df['ImageId'].unique().shape
```

```{python}
def absol2relat(rle):
    if rle == '-1': return '-1'
    pixels = rle.split()
    new_rle = []
    cur = 0
    for k in range(0, len(pixels), 2):
        if k==0:
            new_rle.append(pixels[k])
            new_rle.append(pixels[k+1])
        else:
            cur = int(pixels[k])
            prev = int(pixels[k-2])+int(pixels[k-1])
            new_rle.append(str(cur-prev))
            new_rle.append(pixels[k+1])
    return ' '.join(new_rle)
```

```{python}
def relat2absol(rle):
    if rle == '-1': return '-1'
    pixels = rle.split()
    new_rle = []
    cur = 0
    for k in range(0, len(pixels), 2):
        pix = pixels[k]
        cur += int(pix)
        length = pixels[k+1]
        new_rle.append(str(cur))
        new_rle.append(length)
        cur += int(length)
    return ' '.join(new_rle)
```

```{python}
def merge_rles(rle1, rle2):
    if rle1 == rle2: return rle1
    i1 = 0
    i2 = 0
    rle = []
    pixels1 = relat2absol(rle1).split()
    pixels2 = relat2absol(rle2).split()
    while i1<len(pixels1) and i2<len(pixels2):
        p1 = int(pixels1[i1])
        l1 = int(pixels1[i1+1])
        p2 = int(pixels2[i2])
        l2 = int(pixels2[i2+1])
        if p1<=p2: 
            rle.append(str(p1))
            if p2<=p1+l1-1:
                rle.append(str(max(p2-p1+l2, l1)))
                i2 += 2
            else:
                rle.append(str(l1))
            i1 += 2
        else: 
            rle.append(str(p2))
            if p1<=p2+l2-1:
                rle.append(str(max(p1-p2+l1, l2)))
                i1 += 2
            else:
                rle.append(str(l2))
            i2 += 2
            
    rle += pixels1[i1:]+pixels2[i2:]
    return absol2relat(' '.join(rle))
```

```{python}
def merge_doubles(old, new):
    df = pd.read_csv(old)
    new_df = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])
    for k, id in enumerate(df['ImageId'].unique()):
        new_rle = ''
        for rle in df.loc[df['ImageId']==id, 'EncodedPixels']:
            new_rle = merge_rles(new_rle, rle)
        new_df.loc[k] = [id, new_rle]
    new_df.to_csv(new, index=False)
```

```{python}
#merge_doubles(PROJECT_PATH/'data/train-rle-fastai.csv', PROJECT_PATH/'data/train-rle-fastai2.csv')
```

```{python}
def keep_pos(old, new):
    df = pd.read_csv(old)
    new_df = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])
    for row in df.itertuples():
        id = row.ImageId
        rle = row.EncodedPixels
        k = row.Index
        if rle != '-1' or random.random() <= 0.1:
            # keep all postive and only 10% of negative for segmentation
            new_df.loc[k] = [id, rle]
    new_df.to_csv(new, index=False)
```

```{python}
#keep_pos(LABELS, LABELS_POS)
```

```{python}
def create_classif_csv(old, new):
    df = pd.read_csv(old)
    new_df = pd.DataFrame(columns=['ImageId', 'Labels'])
    for row in df.itertuples():
        image_id = row.ImageId
        rle = row.EncodedPixels
        new_df.loc[row.Index] = [image_id, 1 if rle!='-1' else 0]
    new_df.to_csv(new, index=False)
```

```{python}
create_classif_csv(LABELS, LABELS_CLASSIF)
```

```{python}
def open_image(fn):
    return pydicom.dcmread(str(fn)).pixel_array

def show(img, figsize=(10, 10)):
    plt.figure(figsize=figsize)
    plt.axis('off')
    plt.imshow(img, cmap=plt.cm.bone)
    plt.show()
```

```{python}
def show_dcm_info(fn):
    dataset = pydicom.dcmread(str(fn))
    print("Filename.........:", fn)
    print("Storage type.....:", dataset.SOPClassUID)
    print()

    pat_name = dataset.PatientName
    display_name = pat_name.family_name + ", " + pat_name.given_name
    print("Patient's name......:", display_name)
    print("Patient id..........:", dataset.PatientID)
    print("Patient's Age.......:", dataset.PatientAge)
    print("Patient's Sex.......:", dataset.PatientSex)
    print("Modality............:", dataset.Modality)
    print("Body Part Examined..:", dataset.BodyPartExamined)
    print("View Position.......:", dataset.ViewPosition)
    
    if 'PixelData' in dataset:
        rows = int(dataset.Rows)
        cols = int(dataset.Columns)
        print("Image size.......: {rows:d} x {cols:d}, {size:d} bytes".format(
            rows=rows, cols=cols, size=len(dataset.PixelData)))
        if 'PixelSpacing' in dataset:
            print("Pixel spacing....:", dataset.PixelSpacing)
```

```{python}
fn = next(TRAIN_PATH.glob('**/*.dcm'))
img = open_image(fn)
show(img)
```

```{python}
show_dcm_info(fn)
```

```{python}
img.shape
```

```{python}
def mask2rle(img, width, height):
    rle = []
    lastColor = 0
    currentPixel = 0
    runStart = -1
    runLength = 0

    for x in range(width):
        for y in range(height):
            currentColor = img[x][y]
            if currentColor != lastColor:
                if currentColor == 255:
                    runStart = currentPixel
                    runLength = 1
                else:
                    rle.append(str(runStart))
                    rle.append(str(runLength))
                    runStart = -1
                    runLength = 0
                    currentPixel = 0
            elif runStart > -1:
                runLength += 1
            lastColor = currentColor
            currentPixel += 1

    return " ".join(rle)
```

```{python}
def rle2mask(rle, width, height):
    if rle == '-1':
        return np.zeros((width, height))
    mask = np.zeros(width * height)
    array = np.asarray([int(x) for x in rle.split()])
    starts = array[0::2]
    lengths = array[1::2]

    current_position = 0
    for index, start in enumerate(starts):
        current_position += start
        mask[current_position:current_position+lengths[index]] = 255
        current_position += lengths[index]

    return mask.reshape(width, height).T
```

```{python}
class PneumoSegmentationList(SegmentationItemList):
    def open(self, fn):
        x = open_image(fn)
        x = pil2tensor(x, np.float32)
        x = torch.cat((x, x, x))
        return Image(x/255)
```

```{python}
class ImageSegmentFloat(ImageSegment):
    @property
    def data(self):
        return self.px.float()
```

```{python}
class MaskList(SegmentationLabelList):
    def __init__(self, *args, train_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.train_path = train_path
        
    def open(self, fn):
        assert self.train_path, "a path for train set must be specified"
        img_path = fn[0]
        rle = fn[1]
        h, w = open_image(self.train_path/img_path).shape
        y = rle2mask(rle, w, h)
        y = pil2tensor(y, np.float32)
        return ImageSegmentFloat(y/255)
    
    def analyze_pred(self, pred, thresh: float = 0.5):
        return (pred > thresh).float()
    
    def reconstruct(self, t):
        return ImageSegmentFloat(t.float())
```

```{python}
class PneumoClassifList(ImageList):
    def open(self, fn):
        x = open_image(fn)
        x = pil2tensor(x, np.float32)
        x = torch.cat((x, x, x))
        return Image(x/255)
```

```{python}
def get_weights(train_list):
    df = train_list.inner_df
    n_tot = df.shape[0]
    df = df.reindex(index=range(n_tot), method='bfill')
    class_weights = []
    weights = np.zeros(n_tot)    
    for c in train_list.classes:
        w = df.loc[df['Labels']==c].shape[0]/n_tot
        w = (1-w)/(train_list.c-1)
        class_weights.append(w)
        weights[df.loc[df['Labels']==c].index.values] = w
    return weights, class_weights
```

```{python}
class WeightList:
    def __init__(self, counter, classes=[]):
        assert isinstance(classes, list), "classes must be lists"
        self._counter = counter
        self._classes = classes
        
    def __len__(self):
        return len(self._classes)
    
    def __getitem__(self, key):
        return self._counter[self._classes[key]]
    
    def __iter__(self):
        return iter([self._counter[c] for c in self._classes])
        
    def append(self, c):
        self._classes.append(c)
        
    def pop(self, key):
        self._classes.pop(key)
        
    def increment(self, c):
        self._counter[c] += 1
        
    def inverse(self):
        self._counter = 1/self._counter
        
    def tolist(self):
        return [self._counter[c] for c in self._classes]
```

```{python}
def create_sampler(train_list, class_weights):
    weights = [class_weights[c.data] for _, c in train_list.train]
    sampler = WeightedRandomSampler(weights, len(weights))
    return sampler
```

```{python}
def load_data(path, bs=8, train_size=256):
    train_list = (PneumoSegmentationList.
                  from_csv(path.parent, path.name).
                  split_by_rand_pct(valid_pct=0.2).
                  label_from_df(cols=[0, 1], classes=['pneum'], label_cls=MaskList, train_path=path.parent).
                  transform(get_transforms(), size=train_size, tfm_y=True).
                  databunch(bs=bs, num_workers=0).
                  normalize())
    return train_list
```

```{python}
def load_data_classif(path, bs=8, train_size=256, weight_sample=True):
    train_list = (PneumoClassifList.
                  from_csv(path.parent, path.name).
                  split_by_rand_pct(valid_pct=0.2).
                  label_from_df().
                  transform(get_transforms(), size=train_size))
    if weight_sample:
        weights, class_weights = get_weights(train_list)
        sampler = WeightedRandomSampler(weights, len(weights))
        
    train_list = train_list.databunch(bs=bs, num_workers=0).normalize()
    
    if weight_sample:
        train_list.train_dl = train_list.train_dl.new(shuffle=False, sampler=sampler)
        return train_list, class_weights
    
    return train_list, None
```

```{python}
db = load_data(LABELS_POS, bs=BATCH_SIZE, train_size=TRAIN_SIZE)
```

```{python}
db.show_batch(rows=3, cmap=plt.cm.bone)
```

```{python}
def dice(input, target, thr=None, smooth=1., reduction='mean'):
    assert input.shape==target.shape, "input and target must have same shape"
    iflat = torch.sigmoid(input).view(input.size(0), -1)
    if thr: iflat = (iflat>thr).float()
    tflat = target.view(target.size(0), -1)
    
    intersection = (iflat * tflat).sum(-1)
    dice = (2. * intersection + smooth)/(iflat.sum(-1) + tflat.sum(-1) + smooth)
    
    if reduction=='mean':
        return dice.mean()
    elif reduction=='sum':
        return dice.sum()
    else:
        return dice
```

```{python}
def dice_loss(input, target, smooth=1e-2, **kwargs):
    return 1-dice(input, target, smooth=smooth, **kwargs)
```

```{python}
def bce_dice_loss(input, target, a=0.5, b=0.5, smooth=1., weights=None, **kwargs):
    return a*dice_loss(input, target, smooth=smooth, **kwargs)+b*binary_cross_entropy_with_logits(input, target, pos_weight=weights, **kwargs)
```

```{python}
class DiceLoss(nn.Module):
    def __init__(self, smooth=1., reduction='mean'):
        super().__init__()
        self.smooth = smooth
        self.reduction = reduction
    
    def forward(self, input, target):
        return dice_loss(input, target, smooth=self.smooth, reduction=self.reduction)
```

```{python}
class BCEDiceLoss(nn.Module):
    def __init__(self, a=0.5, b=0.5, smooth=1., weights=None, reduction='mean'):
        super().__init__()
        self.a = a
        self.b = b
        self.smooth = smooth
        self.weights = weights
        self.reduction = reduction
        
    def forward(self, input, target, **kwargs):
        return bce_dice_loss(input, target, a=self.a, b=self.b, smooth=self.smooth, weights=self.weights, reduction=self.reduction)
```

```{python}
class RandomSampler(Sampler):
    def __init__(self, model, loss, dl, num_samples, bs):
        assert loss.reduction=='none', 'Loss function must disable reduction'
        self.model = model
        self.bs = bs
        self.dl = dl.new(shuffle=False, sampler=None)
        self.loss = loss
        self.num_samples = num_samples

    def get_scores(self):
        losses = []
        with torch.no_grad():
            for X, y_true in self.dl:
                y_pred = self.model(X)
                loss = self.loss(y_pred, y_true)
                losses.append(loss)
        return torch.cat(losses)

    def __iter__(self):
        scores = self.get_scores()
        return iter(torch.multinomial(scores, self.num_samples, True).tolist())

    def __len__(self):
        return self.num_samples
```

```{python}
learner = unet_learner(db, models[MODEL], pretrained=PRETRAINED, loss_func=BCEDiceLoss(a=0.8, b=0.2), wd=WD, model_dir=MODELS_PATH, metrics=[dice])
```

```{python}
next(learner.model.children())[0].load_state_dict(torch.load(STATE_DICT_PATH))
```

```{python}
learner.to_fp16()
```

```{python}
learner.lr_find()
```

```{python}
learner.recorder.plot(skip_end=1)
```

```{python}
learner.fit_one_cycle(
    10, slice(LR),
    callbacks=[
        SaveModelCallback(
            learner, monitor='dice', name='seg_resnet152_0')])
```

```{python}
learner.show_results()
```

```{python}
learner.unfreeze()
```

```{python}
learner.lr_find(num_it=2000)
```

```{python}
learner.recorder.plot(skip_end=1)
```

```{python}
learner.fit_one_cycle(
    10, slice(1e-2),
    callbacks=[
        SaveModelCallback(
            learner, monitor='dice', name='first_test_unfrozen')])
```

```{python}
test_list = (PneumoSegmentationList.
             from_folder(TEST_PATH, extensions=['.dcm']))
db.add_test(test_list, tfms=(), tfm_y=False)
```

```{python}
preds = learner.get_preds(DatasetType.Test)
```

```{python}
preds = preds[0]
```

```{python}
preds = torch.sigmoid(preds)
```

```{python}
valid_preds, valid_gt = learner.get_preds(DatasetType.Valid)
```

```{python}
def get_best_thr(preds, gt, plot=True):
    thrs = np.arange(0.01, 1, 0.01)
    dices = []
    for i in tqdm(thrs):
        dices.append(dice(preds_m, valid_gt, thr=i).item())
    dices = np.array(dices)
    best_dice = dices.max()
    best_thr = thrs[dices.argmax()]
    
    if plot:
        plt.figure(figsize=(8,4))
        plt.plot(thrs, dices)
        plt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())
        plt.text(best_thr+0.03, best_dice-0.01, f'DICE = {best_dice:.3f}', fontsize=14);
        plt.show()
    
    return best_thr
```

```{python}
def create_submission(preds, path, thr=0.5):
    sub = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])
    index = 0
    for k, pred in tqdm(enumerate(preds.squeeze()), total=preds.size(0)):
        y = pred.numpy()
        y = cv2.resize(y, (1024, 1024), interpolation=cv2.INTER_CUBIC)
        labels = label(y>thr)
        id = db.test_ds.items[k].with_suffix('').name
        if labels.max() == 0:
            sub.loc[index] = [id, '-1']
            index += 1
        for i in range(1, labels.max()+1):
            mask = (labels==i).astype(np.uint8)*255
            rle = mask2rle(mask.T, 1024, 1024)
            sub.loc[index] = [id, rle]
            index += 1
    sub.to_csv(path, index=False)
    return sub
```

```{python}
db_clf, class_weights = load_data_classif(LABELS_CLASSIF, bs=BATCH_SIZE, train_size=TRAIN_SIZE, weight_sample=True)
```

```{python}
clf = cnn_learner(db_clf, models['resnet152'], pretrained=PRETRAINED,
                  loss_func=nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=db_clf.device)),
                  wd=WD, model_dir=MODELS_PATH, metrics=[accuracy])
```

```{python}
loss =  nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=db_clf.device), reduction='none')
sampler = RandomSampler(clf.model, loss, db_clf.train_dl, len(db_clf.train_ds), db_clf.batch_size)
```

```{python}
clf.data.train_dl = db_clf.train_dl.new(shuffle=False, sampler=sampler)
```

```{python}
db_clf.show_batch(cmap=plt.cm.bone)
```

```{python}
clf.unfreeze()
```

```{python}
clf.load('clf_resnet152_0').to_fp16()
```

```{python}
clf.lr_find(num_it=500)
```

```{python}
clf.recorder.plot()
```

```{python}
clf.fit_one_cycle(
    50, slice(3e-5),
    callbacks=[
        SaveModelCallback(
            clf, monitor='accuracy', name='clf_resnet152_2')])
```

```{python}
clf.recorder.plot_losses()
```

```{python}
clf.recorder.plot_metrics()
```

```{python}
clf.show_results()
```

```{python}
interp = clf.interpret()
```

```{python}
interp.plot_confusion_matrix()
```

```{python}
interp.plot_top_losses(9, heatmap=False)
```

```{python}
df = pd.read_csv(LABELS_CLASSIF)
stats = pd.DataFrame(columns=['ImageId', 'Age', 'Sex', 'Height', 'Width', 'Pneumo'])
for row in tqdm(df.itertuples(), total=df.shape[0]):
    fn = row.ImageId
    ds = pydicom.dcmread(str(DATA/fn))
    h, w = ds.pixel_array.shape
    stats.loc[row.Index] = [row.ImageId, int(ds.PatientAge), ds.PatientSex, h, w, int(row.Labels)]
```

```{python}
stats.head()
```

```{python}
stats.loc[stats['Sex']=='F'].shape[0]
```

```{python}
stats.loc[stats['Sex']=='M'].shape[0]
```

```{python}
stats_age = stats.groupby(pd.cut(stats['Age'], list(range(0, 101, 10))+[450]))
```

```{python}
stats_age.count()['Pneumo']
```

```{python}
stats_age.sum()['Pneumo']/stats_age.count()['Pneumo']
```

```{python}
stats_sex = stats.groupby('Sex')
```

```{python}
stats_sex.count()['Pneumo']
```

```{python}
stats_sex.sum()['Pneumo']/stats_sex.count()['Pneumo']
```

```{python}
losses, idxs = interp.top_losses()
```

```{python}
idxs = idxs.numpy()
```

```{python}
ids = []
for i, l in zip(idxs, losses):
    if l.item()<0.2:
        break
    id = Path(interp.ds.items[i])
    id = Path(id.parent.name)/id.name
    ids.append(str(id))
```

```{python}
top_losses = stats.loc[stats['ImageId'].isin(ids)]
```

```{python}
top_losses.head()
```

```{python}
top_losses.groupby('Pneumo').count().head()['ImageId']
```

```{python}
top_losses.groupby('Sex').count().head()['ImageId']/stats_sex.count()['Pneumo']
```

```{python}
top_losses.groupby(pd.cut(top_losses['Age'], list(range(0, 101, 10))+[450])).count().head()['ImageId']/stats_age.count()['Pneumo']
```

```{python}
torch.save(next(clf.model.children()).state_dict(), MODELS_PATH/'resnet152_backbone_pretrained.pth')
```

```{python}
clf.destroy()
```

```{python}

```
